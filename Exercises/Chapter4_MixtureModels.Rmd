---
title: "Chapter4_MixtureModels"
output: html_document
---

Install packages:

bootstrap
flexmix
ggbeeswarm
mixtools
mosaics (Bioconductor)
mclust
HistData

```{r setup, include=FALSE}
library(dplyr)
library(here)
library(ggplot2)
library(vcd)
library(bootstrap)
library(RColorBrewer)
library(flexmix)
library(mixtools)
library(ggbeeswarm)
library(mosaics)
library(mosaic)
library(mosaicsExample)
library(HistData)

knitr::opts_chunk$set(echo = TRUE)
```
## Finite Mixtures

Example of a Mixture Model with two equal-sized components.

```{r}
coinflips = (runif(10000) > 0.5)
table(coinflips)
oneFlip = function(fl, mean1 = 1, mean2 = 3, sd1 = 0.5, sd2 = 0.5) {
  if (fl) {
   rnorm(1, mean1, sd1)
  } else {
   rnorm(1, mean2, sd2)
  }
}
fairmix = vapply(coinflips, oneFlip, numeric(1))
library("ggplot2")
library("dplyr")
ggplot(tibble(value = fairmix), aes(x = value)) +
     geom_histogram(fill = "purple", binwidth = 0.1)
```
**Question 4.1**
How can you use R's vectorized syntax to remove the vapply loop?

Solution 4.1
You can use ifelse function

```{r}
means = c(1, 3)
sds   = c(0.25, 0.25)
values = rnorm(length(coinflips),
          mean = ifelse(coinflips, means[1], means[2]),
          sd   = ifelse(coinflips, sds[1],   sds[2]))
values
```

**Queston 4.2**
Perform one million coin flips with the improved code above and make a histogram with 500 bins.  What do you notice?

```{r}
fair = tibble(
  coinflips = (runif(1e6) > 0.5),
  values = rnorm(length(coinflips),
               mean = ifelse(coinflips, means[1], means[2]),
               sd   = ifelse(coinflips, sds[1],   sds[2])))
ggplot(fair, aes(x = values)) +
     geom_histogram(fill = "purple", bins = 500)
```
The histograms become much smoother as the number of observations and bins increases.

**Question 4.3**
(a) Plot a histogram of fair$values where coinflips is TRUE
(b) Overlay the line corresponding to $\phi$(**z**)

```{r}
ggplot(filter(fair, coinflips), aes(x = values)) +
   geom_histogram(aes(y = ..density..), fill = "purple",
                  binwidth = 0.01) +
   stat_function(fun = dnorm,
          args = list(mean = means[1], sd = sds[1]), color = "red")
```

Note:  In the code chunk above, ...density... is used to show the proportions of counts.

Example of the bimodal distribution

```{r}
fairtheory = tibble(
  x = seq(-1, 5, length.out = 1000),
  f = 0.5 * dnorm(x, mean = means[1], sd = sds[1]) +
      0.5 * dnorm(x, mean = means[2], sd = sds[2]))
ggplot(fairtheory, aes(x = x, y = f)) +
  geom_line(color = "red", size = 1.5) + ylab("mixture density")
```

**Question 4.4**
Can you guess the two mean parameters of the component distribution for Figure 4.4 using trial and error?


```{r}
coinflips2 = (runif(1000) > 0.5)
table(coinflips2)
oneFlip2 = function(fl, mean1 = 0.9, mean2 = 2, sd1 = 0.25, sd2 = 0.25) {
  if (fl) {
   rnorm(1, mean1, sd1)
  } else {
   rnorm(1, mean2, sd2)
  }
} #Altered funtion for mean1 and mean2 with value range of 0.5-5
fairmix2 = vapply(coinflips2, oneFlip2, numeric(1))
ggplot(tibble(value = fairmix), aes(x = value)) +
     geom_histogram(fill = "purple", binwidth = 0.1)

```
Despite several attempts, could not replicate Figure 4.4 using trial and error for the means commented in the R code chunk above...

Chapter Example for Figure 4.4
```{r}
mystery = tibble(
  coinflips = (runif(1e3) > 0.5),
  values = rnorm(length(coinflips),
               mean = ifelse(coinflips, 1, 2),
               sd   = ifelse(coinflips, sqrt(.5), sqrt(.5))))
br2 = with(mystery, seq(min(values), max(values), length.out = 30))
ggplot(mystery, aes(x = values)) +
geom_histogram(fill = "purple", breaks = br2)
```
Solution 4.4

```{r}
head(mystery, 3)
br = with(mystery, seq(min(values), max(values), length.out = 30))
ggplot(mystery, aes(x = values)) +
  geom_histogram(data = dplyr::filter(mystery, coinflips),
     fill = "red", alpha = 0.2, breaks = br) +
  geom_histogram(data = dplyr::filter(mystery, !coinflips),
     fill = "darkblue", alpha = 0.2, breaks = br)
```
Note: Red = points generated from heads coin flipes and Blue = points generated from tails coin flips.

The components can also be plotted in a different way to disentangle them...

```{r}
ggplot(mystery, aes(x = values, fill = coinflips)) +
  geom_histogram(breaks = br, alpha = 0.2) +
  scale_fill_manual(values = c(`TRUE` = "red", `FALSE` = "darkblue"),
                    guide = FALSE)
```
## Discovering the Hidden Group Labels

Expectation-maximization (EM) algorithm
- Infer hidden groups that are mixed together in the data

Bivariate Distribution
- Distribution of "couples"

**Question 4.5**

```{r}
probHead = c(0.125, 0.25)
set.seed(122)
for (pi in c(1/8, 1/4)) {
  whCoin = sample(2, 100, replace = TRUE, prob = c(pi, 1-pi))
  K = rbinom(length(whCoin), size = 2, prob = probHead[whCoin])
  print(table(K))
}
```
Suppose we have a mixture of two normals with $\mu_1$ = ?, $\mu_2$ = ?, $\theta_$ = $\theta_2$ = 1

```{r}
set.seed(122)
mus = c(-0.5, 1.5)
u = sample(2, 100, replace = TRUE)
y = rnorm(length(u), mean = mus[u])
duy = tibble(u, y)
head(duy)
```

```{r}
group_by(duy, u) %>% summarize(mean(y))
```
**Question 4.6**
Try writing the loglikelihood function for a mixture model..

Still need to work on this...

**Question 4.7**
Run the EM function several times with different starting values

```{r}
library(mclust)

y = c(rnorm(100, mean = -0.2, sd = 0.5),
      rnorm( 50, mean =  0.5, sd =   1))
fit = Mclust(y)
summary(fit)

gm = normalmixEM(y, k = 2, lambda = c(0.5, 0.5),
     mu = c(-0.01, 0.01), sigma = c(1, 1))
gm$lambda
gm$mu
gm$sigma
gm$loglik

```
## Models for Zero-inflated Data

ChIP-Seq Data
```{r}
vignette(package = "mosaics")
```

How the binTFBS data was created...
```{r}
library("mosaics")
library("mosaicsExample")

constructBins( infile=system.file( file.path("extdata","wgEncodeSydhTfbsGm12878Stat1StdAlnRep1_chr22_sorted.bam"), package="mosaicsExample"), 
    fileFormat="bam", outfileLoc="./", 
    byChr=FALSE, useChrfile=FALSE, chrfile=NULL, excludeChr=NULL, 
    PET=FALSE, fragLen=200, binSize=200, capping=0 )
constructBins( infile=system.file( file.path("extdata","wgEncodeSydhTfbsGm12878InputStdAlnRep1_chr22_sorted.bam"), package="mosaicsExample"), 
    fileFormat="bam", outfileLoc="./", 
    byChr=FALSE, useChrfile=FALSE, chrfile=NULL, excludeChr=NULL, 
    PET=FALSE, fragLen=200, binSize=200, capping=0 )

datafiles = c("/MBIO7160/ModernStatsforModernBiol/RData/wgEncodeSydhTfbsGm12878Stat1StdAlnRep1_chr22_sorted.bam_fragL200_bin200.txt",
              "/MBIO7160/ModernStatsforModernBiol/RData/wgEncodeSydhTfbsGm12878InputStdAlnRep1_chr22_sorted.bam_fragL200_bin200.txt")
binTFBS <- readBins( type=c("chip","input"), fileName = datafiles )

```

What does the data look like?
```{r}
binTFBS
```
Create a histogram of per-bin counts

```{r}
bincts = print(binTFBS)
ggplot(bincts, aes(x = tagCount)) +
  geom_histogram(binwidth = 1, fill = "forestgreen")
```
It is not immediately obvious whether the number of zeros is really extreme...

Redo histogram with a log10 scale for the y-axis
```{r}
plot = ggplot(bincts, aes(x = tagCount)) + scale_y_log10() +
   geom_histogram(binwidth = 1, fill = "forestgreen")

summary(bincts) # Display summary stats
table(bincts$tagCount == 0) # Display the number of bins with zero counts

```
The proportion of bins with zero counts = 111679 / 256523 

## More than two components

Simulation of 7000 nucleotide mass measurements.

```{r}
masses = c(A =  331, C =  307, G =  347, T =  322)
probs  = c(A = 0.12, C = 0.38, G = 0.36, T = 0.14)
N  = 7000
sd = 3
nuclt   = sample(length(probs), N, replace = TRUE, prob = probs)
quadwts = rnorm(length(nuclt),
                mean = masses[nuclt],
                sd   = sd)
ggplot(tibble(quadwts = quadwts), aes(x = quadwts)) +
  geom_histogram(bins = 100, fill = "purple")
```
**Question 4.9**

Repeat the simulation above with N=1000 nucleotide measurments.  What do you notice?

```{r}
masses = c(A =  331, C =  307, G =  347, T =  322)
probs  = c(A = 0.12, C = 0.38, G = 0.36, T = 0.14)
N  = 1000 # Modify number of simulations
sd = 3
nuclt   = sample(length(probs), N, replace = TRUE, prob = probs)
quadwts = rnorm(length(nuclt),
                mean = masses[nuclt],
                sd   = sd)
ggplot(tibble(quadwts = quadwts), aes(x = quadwts)) +
  geom_histogram(bins = 100, fill = "purple")
```
The curvature of the historgrams appears less smooth.

**Question 4.10**

What about when N=7000 and standard deviation is 10?

```{r}
masses = c(A =  331, C =  307, G =  347, T =  322)
probs  = c(A = 0.12, C = 0.38, G = 0.36, T = 0.14)
N  = 7000
sd = 10
nuclt   = sample(length(probs), N, replace = TRUE, prob = probs)
quadwts = rnorm(length(nuclt),
                mean = masses[nuclt],
                sd   = sd)
ggplot(tibble(quadwts = quadwts), aes(x = quadwts)) +
  geom_histogram(bins = 100, fill = "purple") 
```
The distinction between the distributions is no longer obvious.

**Question 4.11**

Plot the theoretical density curve for the distribution simulated in Figure 4.9.

```{r}
masses = c(A =  331, C =  307, G =  347, T =  322)
probs  = c(A = 0.12, C = 0.38, G = 0.36, T = 0.14)
N  = 7000
sd = 3
nuclt   = sample(length(probs), N, replace = TRUE, prob = probs)
mean1 = masses[nuclt]
mean1
quadwts = rnorm(length(nuclt),
                mean = masses[nuclt],
                sd   = sd)
ggplot(tibble(quadwts = quadwts), aes(x = quadwts)) +
  geom_histogram(aes(y = ..density..), bins = 100, fill = "purple") +
  geom_density(color = "red") # Add geom_density to code to plot smooth curve
```

## Empirical Distributions and the Non-parametric Bootstrap

Extreme case of the mixture model; model our sample of *n* data points as a mixture of *n* point masses

Use Darwin's *Zea mays* plant data

```{r}
library("HistData")
ZeaMays$diff
ggplot(ZeaMays, aes(x = diff, ymax = 1/15, ymin = 0)) +
  geom_linerange(size = 1, col = "forestgreen") + ylim(0, 0.1)
```
Bootstrap sampling distribution of the median of the *Zea mays* differences

```{r}
B = 1000
meds = replicate(B, {
  i = sample(15, 15, replace = TRUE)
  median(ZeaMays$diff[i])
})
ggplot(tibble(medians = meds), aes(x = medians)) +
  geom_histogram(bins = 30, fill = "purple")
```
**Question 4.12**
Estimate a 99% confidence interval for the median based on the simulations.

Need to fix code below...

```{r}
?qdata

Q99 = qdata(meds, c(0.01,.99)) #qdata is from mosaic pkg
Q99

hist(Q99$quantile)

abline(v = Q99$Q99, col = "blue" ,lwd = 3)
```
**Question 4.13**
Use the bootstrap package to redo the analysis.

Need to revist this...

```{r}
bootmean = bootstrap(ZeaMays$diff, B, mean)
bootmedian = bootstrap(ZeaMays$diff, B, median)
```
**Question 4.14**
If a sample is composed of n = 3 different values, how many different bootstrap resamples are possible?  What about n = 15?

```{r}
c(N3 = choose(5, 3), N15 = choose(29, 15)) 
# 5 and 29 arise from 2n - 1
```

## Infinite Mixtures

When the number of mixture components is as big as (or bigger than) the number of observations.

Histogram of data generated from a symmetric Laplace distribution.

```{r}
w = rexp(10000, rate = 1)

mu  = 0.3
lps = rnorm(length(w), mean = mu, sd = sqrt(w))
ggplot(data.frame(lps), aes(x = lps)) +
  geom_histogram(fill = "purple", binwidth = 0.1)
```

## Asymmetric Laplace

Histogram of data generated from an asymmetric Laplace distribution.

```{r}
mu = 0.3; sigma = 0.4; theta = -1
w  = rexp(10000, 1)
alps = rnorm(length(w), theta + mu * w, sigma * sqrt(w))
ggplot(tibble(alps), aes(x = alps)) +
  geom_histogram(fill = "purple", binwidth = 0.1)
```

## Gamma-Poisson Mixture

Relies on two parameters; shape and scale

```{r}
ggplot(tibble(x = rgamma(10000, shape = 2, rate = 1/3)),
   aes(x = x)) + geom_histogram(bins = 100, fill= "purple")
ggplot(tibble(x = rgamma(10000, shape = 10, rate = 3/2)),
   aes(x = x)) + geom_histogram(bins = 100, fill= "purple")
```
Generate a Gamma-Poisson Distribution

```{r}
lambda = rgamma(10000, shape = 10, rate = 3/2)
gp = rpois(length(lambda), lambda = lambda)
ggplot(tibble(x = gp), aes(x = x)) +
  geom_histogram(bins = 100, fill= "purple")
```

**Question 4.18**
(a) Are the values generated from the Gamma-Poisson mixture continuous or discrete?

Discrete; has a countable number of possible values

(b) What is another name for this distribution?

Negative binomial distribution

Note
Binomial counts successes in a fixed number of trials, while Negative binomial counts failures until a fixed number successes.

Goodness-of-fit Plot
```{r}
library("vcd")
ofit = goodfit(gp, "nbinomial")
plot(ofit, xlab = "")
ofit$par
```
The rootogram shows the theoretical probabilities of the gamma-Poisson distribution as red dots and the square roots of the observed frequencies as the height of the rectangular bars.  Since the bars align well with the horizontal axis, the negative binomial appears to be a good fit.

Visualization of the Hierarchical Model that generated the Gamma-Poisson Distribution

```{r}
# set-up Gamma-Poisson
x    = 0:95
mu   = 50
vtot = 80
v1   = vtot - mu
scale = v1/mu    # 0.6
shape = mu^2/v1  # 83.3
p1   = dgamma(x = x, scale = 0.6, shape = 80)
p2   = dpois(x = x, lambda = mu*1.2)
p3   = dnbinom(x = x, mu = mu, size = mu^2/vtot)

#Plot distribution
library("RColorBrewer")
cols = brewer.pal(8, "Paired")
par(mfrow=c(3,1), mai=c(0.5, 0.5, 0.01, 0.01))
xlim = x[c(1, length(x))]
plot(NA, NA, xlim=xlim, ylim=c(0,0.07), type="n", ylab="", xlab="")
polygon(x, p1, col=cols[1])
abline(v=mu, col="black", lwd=3)
abline(v=mu*1.2, col=cols[2], lty=2, lwd=3)
plot(x, p2, col=cols[3], xlim=xlim, ylab="", xlab="", type="h", lwd=2)
abline(v=mu*1.2, col=cols[2], lwd=2)
abline(v=mu*1.1, col=cols[4], lty=2, lwd=3)
plot(x, p3, col=cols[4], xlim=xlim, type="h", lwd=2, ylab="", xlab="")
```

## Variance-stabilizing transformations

Variance stabilizing transformations can help control for varaibility between repeated measurements of the same underlying true value in the dataset.

```{r}
lambdas = seq(100, 900, by = 100)
simdat = lapply(lambdas, function(l)
    tibble(y = rpois(n = 40, lambda=l), lambda = l)
  ) %>% bind_rows
simdat
library("ggbeeswarm")
ggplot(simdat, aes(x = lambda, y = y)) +
  geom_beeswarm(alpha = 0.6, color = "purple")
ggplot(simdat, aes(x = lambda, y = sqrt(y))) +
  geom_beeswarm(alpha = 0.6, color = "purple")
```
The data display heteroscedasticity; the standard deviations (or variance) are different in different regions.  They increase along the x-axis with the mean.

Heteroscedasticity is harder to see with fewer replicates.

Transform the data to have approximately the same variance

```{r}
summarise(group_by(simdat, lambda), sd(y), sd(2*sqrt(y)))
```

**Question 4.20**
Repeat the computation for a version of simdat with a number of replicates larger than 40.

```{r}
lambdas = seq(100, 900, by = 100)
simdat = lapply(lambdas, function(l)
    tibble(y = rpois(n = 100, lambda=l), lambda = l) #Increased n
  ) %>% bind_rows
simdat
library("ggbeeswarm")
ggplot(simdat, aes(x = lambda, y = y)) +
  geom_beeswarm(alpha = 0.6, color = "purple")
ggplot(simdat, aes(x = lambda, y = sqrt(y))) +
  geom_beeswarm(alpha = 0.6, color = "purple")

summarise(group_by(simdat, lambda), sd(y), sd(2*sqrt(y))) #transform data to have similar variance
```
Variance is now very close to 1.

Generate the gamma-Poisson varaibles u and plot the 95% confidence intervals around the mean.

```{r}
muvalues = 2^seq(0, 10, by = 1)
simgp = lapply(muvalues, function(mu) {
  u = rnbinom(n = 1e4, mu = mu, size = 4)
  tibble(mean = mean(u), sd = sd(u),
         lower = quantile(u, 0.025),
         upper = quantile(u, 0.975),
         mu = mu)
  } ) %>% bind_rows
head(as.data.frame(simgp), 2)
ggplot(simgp, aes(x = mu, y = mean, ymin = lower, ymax = upper)) +
  geom_point() + geom_errorbar()
```

Piecewise-linear function to stablize the variance of the data

```{r}
simgp = mutate(simgp,
  slopes = 1 / sd,
  trsf   = cumsum(slopes * mean))
ggplot(simgp, aes(x = mean, y = trsf)) +
  geom_point() + geom_line() + xlab("")
```
## Exercises

**4.1**
The EM Algorithm
- Generalization of maximum likelihood estimation to the incomplete data case
- Estimate the parameters that maximize the likelihood of the data
  - For example: Given these model parameters, what is the probability that our data X was generated by them?
- The larger the log-likelihood, the better the model parameters fit the data
  - What is large?

Visualize the data
```{r}
yvar = readRDS("/MBIO7160/ModernStatsforModernBiol/RData/Myst.rds")$yvar
yvar
ggplot(tibble(yvar), aes(x = yvar)) + geom_histogram(binwidth=0.025)
str(yvar)
```

Randomly assign a "probability of membership" to each of the values in yvar.

```{r}
pA = runif(length(yvar))
pB = 1 - pA
```
Create housekeeping variables

```{r}
iter = 0
loglik = -Inf
delta = +Inf
tolerance = 1e-3
miniter = 50; maxiter = 1000
```

Study the code below and answer the following questions.

```{r}
while((delta > tolerance) && (iter <= maxiter) || (iter < miniter)) {
  lambda = mean(pA)
  
  #Lines 601-609 are E-Step; estimate our posterior probabilites
  muA = weighted.mean(yvar, pA)
  muB = weighted.mean(yvar, pB)
  sdA = sqrt(weighted.mean((yvar - muA)^2, pA))
  sdB = sqrt(weighted.mean((yvar - muB)^2, pB))

  
  phiA = dnorm(yvar, mean = muA, sd = sdA)
  phiB = dnorm(yvar, mean = muB, sd = sdB)
  
  #Lines 611-616 are M-Step; re-estimating the parameters
  pA   = lambda * phiA
  pB   = (1 - lambda) * phiB
  ptot = pA + pB
  pA   = pA / ptot
  pB   = pB / ptot

  loglikOld = loglik
  loglik = sum(log(pA))
  delta = abs(loglikOld - loglik)
  iter = iter + 1
}
param = tibble(group = c("A","B"), mean = c(muA,muB), sd = c(sdA,sdB))
param
summary(param)
loglik
iter
```
1. What lines correspond to the E-Step and the M-Step?
The lines of code corresponding to each step are indicated in the R code chunk above
- Lines 601-609 are E-Step; estimate our posterior probabilites
- Lines 611-616 are M-Step; re-estimating the parameters

2. What does the M-Step do?  What does the E-Step do?
M-Step
- Re-estimates the model parameters
E-Step
- Missing data are estimated given the observed data and current estimates of model parameters
- Calculates the posterior probabilities

3. What are the roles of the algorithm arguements tolerance, miniter, and maxiter?

Tolerance
- tolerance for the convergence criteria; run the EM algorithm until the log-likehood difference between iterations is < 1e-3 or a maximum of 1000 iterations is met
Miniter
- minimum number of iterations for the EM algorithm to converge; a minimum of 50 iterations are required.
Maxiter
- maximum number of iterations for EM algorithm to converge; a maximum of 1000 iterations are permitted.

4. Why do we need to compute loglik?
We do this to deteremine whether the given model parameters fit our data well.  The larger the log likelihood = better the model parameters fit the data.

With respect to the EM algorithm, the loglik is iteratively calculated until the tolerance is met (i.e., hasn't changed signficantly between iterations, meaning the EM has converged) or the maximum number of iterations is met. 

5. Compare the results to output of normalmixEM function from the *mixtools* package

```{r}
set.seed(122)
EM = normalmixEM(yvar, k = 2, maxit = 1000, epsilon = 1e-3, mu = c(0.147, -0.169), sigma = c(0.150, 0.0983)) # epsilon is = to the tolerance for convergence
summary(gm2)

# Get fewer iterations when mu and sigma are included...
# Do we include mu and sigma in our calculation?  Or, do we just include the tolerance (i.e., epsilon)

set.seed(122)
EM = normalmixEM(yvar, k = 2, maxit = 1000, epsilon = 1e-3) # epsilon is = to the tolerance for convergence
summary(gm2)

#We get more iterations when we don't include them...

```
There are two components, with weights (lambda) of about 0.47 and 0.53, two means (mu), and two standard deviations (sigma). The over-all log-likelihood,obtained after X iterations, is 417.2.

We can compare these values to the mean and sd in param.

It took fewer iterations running the normalmixEM function (i.e., 114 compared to 340)...Is this right?

**4.2**
Use a QQ-plot to compare the theoretical values of the gamma-Poisson distribution in Section 4.4.3 to the data used for the estimation.

```{r}
lambda2 = rgamma(10000, shape = 10, rate = 0.605) # Generate the theoretical values from dataset ofit (page 97), size = 10 and prob = 0.605

qqplot(lambda, lambda2, type = "l", asp = 1) # Compare theoretical values to estimated values (lambda)
abline(0, 1, col = "Red")

qqplot(gp, lambda2, type = "l", asp = 1) # Or do we use the gp dataset?
abline(0, 1, col = "Red")
```

**4.3**

(a) Plot the data and guess how the points were generated.
```{r}
library("flexmix")
data("NPreg") # load data

NPreg

ggplot(NPreg, aes(x = x, y = yn)) + geom_point() # plot the data
```
Points appear to be generated by negative binomial (parabola) and exponential distributions (straight line)...I think that's what they are asking?

Note:
Looking at the graph, it is difficult to distinguish the points that belong to the straight line and those that belong to the parabola; issue of identifiability.

(b) Fit a two-component mixture model using the command below:

```{r}
m1 = flexmix(yn ~ x + I(x^2), data = NPreg, k = 2)
m1

```
(c) Look at the estimated parameters of the mixture components and make a truth table.  What does the summary of m1 show us?

```{r}
# look at the parameters for each component
parameters(m1, component = 1)
parameters(m1, component = 2)
clusters(m1) # Look at the number of clusters with estimated class for each one

#create a truth table of the class and estimated class 
truth = select(NPreg, class)
truth = mutate(truth, estimated_class = clusters(m1))
truth

truth_table = ifelse(truth$class == truth$estimated_class, "TRUE", "FALSE")
truth_table

sum(truth_table == "FALSE") # sum the number of FALSE
sum(truth_table == "TRUE") # sum the number of TRUE

truth2 = mutate(truth, is_equal = class == estimated_class) # Alternative way of making a truth table with all variables
summary(truth2) # look at the summary stats; 190 TRUE, 10 FALSE

table(NPreg$class, clusters(m1)) # R code that accompanied the book; however, doesn't indicate whether 1 OR 2 is TRUE OR FALSE.  Based on the answer above, it is now clear that 1 = TRUE and 2 = FALSE.

summary(m1) # Look at the summary of m1
```
The summary of m1 shows the prior estimates for Comp.1 and Comp.2 as well as the log-likelihood

Note
- BIC = Bayesian Information Critierion
- AIC = Akaike Information Criterion


(d) Plot the data again, this time coloring the points according to its estiamted class.

```{r}
NPreg = mutate(NPreg, gr = factor(class)) # Group points by class
ggplot(NPreg, aes(x = x, y = yn, group = gr)) +
   geom_point(aes(colour = gr, shape = gr)) # plot the data
```

**4.4**

Two papers showing the use of other infinite mixtures for modeling molecular biology technological variation:

BayesFlow: latent modeling of flow cytometry cell populations
Johnsson et al., BMC Bioinformatics (2016)

Gaussian Mixture Model of Heart Rate Variability
Costa et al., PLos ONE (2012)

