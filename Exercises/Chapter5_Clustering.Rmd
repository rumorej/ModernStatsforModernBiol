---
title: "Chapter5_Clustering"
author: "Jill and Abdul-Rahman"
date: "11/05/2019"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
library(here)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(readr)
library(vegan)
library("clusterExperiment")
library("flowCore")
library("scRNAseq")
library("clue")
library(cluster)
library("flowCore")
library("flowViz")
library(pheatmap)
library(RColorBrewer)
library(viridis)
library(mixtools)
library(dada2)
library("cluster")
library("ggcyto")
library("labeling")
library(gridExtra)
library("kernlab")
library("ggmap")
library(factoextra)
library("Hiiragi2013")

knitr::opts_chunk$set(echo = TRUE)
```
## Chapter Goals

1. How to compare observations 
  - Finding the right distance is essential for meaningful results
2. Explore two approaches to clustering
  - Iterative Partioning
    - k-means
    - k-medoids
  - Hierarchical Clustering
3. Explore density based clustering 
  - Flow cytometry/ Mass cytometry data
  - DBSCAN
4. Cluster validation
  - Within groups sum of squares (WSS)
  - Calinski-Harabasz Index
  - Gap Statistic
  - Bootstrapping
5. Distances and probabilities
  - Distances aren't everything
  - Important to consider baseline frequencies and local densities when clustering
  - Essential for denoising 16S rRNA sequence reads

## Clustering

In it's simplest form, clustering aims to group objects that are similiar to one another.  For example, objects in one cluster are considered to be more similar to one another than to objects in another cluster.

Clustering can be useful for highlighting groupings that are important for interpreting the data, which can lead to discoveries, for example, John Snow's map of the cholera cases.

Caveat
  - Clustering algorithms are designed to find clusters
  - Find clusters even where there are none
  - Cluster validation is essential
  
Types of Clustering Discussed in this Chapter

Connectivity models
- Underlying principle = data points that are closer in space exhibit more similarity to each other than the data points lying farther away
- Easy to interpret 
- Lack scalability for handling big datasets
 - Example = Hierarchical Clustering
  - Deteremine the number of appropirate clusters by interpreting the dendrogram

Centroid models
- Iterative clustering algorithms in which the notion of similarity is derived by the closeness of a data point to the centroid of the clusters
- Decide the number of clusters beforehand (i.e., K)
  - Example = *k*-means and *k*-medoids
    - Works well for Big Data

Density Models
- Search the data space for areas of varied density of data points 
- Objects in similar regions of density will be assigned to the same cluster
  - Examples = DBSCAN and OPTICS (for Flow and/or Mass cytometry data)


Not suprisingly, there are a plethora of clustering tools...

Website for clustering tools in BioConductor
https://www.bioconductor.org/packages/devel/BiocViews.html#___Clustering

It is important to keep in mind that clustering is impacted by the chosen distance algorithm

Some examples

Euclidean
Manhattan
Maximum
Weighted Euclidean
Minkowski
Binary
Jaccard
Correlation-based
And many many more...


## Computations related to distances in R

Function below returns a special object of class dist, and computes the binary distance (i.e., one of six choices of distance including euclidean, maximum, manhattan, canberra, binary and minkowski)

```{r}
mx  = c(0, 0, 0, 1, 1, 1)
my  = c(1, 0, 1, 1, 0, 1)
mz  = c(1, 1, 1, 0, 1, 1)
mat = rbind(mx, my, mz)
dist(mat)
dist(mat, method = "binary")
```

To assess the distance between observation 1 and 2, turn the dist class object back into the matrix

```{r}
load("/MBIO7160/ModernStatsforModernBiol/RData/Morder.RData")
Morder
sqrt(sum((Morder[1, ] - Morder[2, ])^2))
as.matrix(dist(Morder))[2, 1]
```
Compute the Jaccard distance

```{r}
mut = read.csv("/MBIO7160/ModernStatsforModernBiol/RData/HIVmutations.csv")
mut[1:3, 10:16]
mut
```

**Question 5.2**
Compare the Jaccard distance between mutations in the HIV dat mut to the correlation-based distance

```{r}
library("vegan")
mutJ = vegdist(mut, "jaccard")
mutC = sqrt(2 * (1 - cor(t(mut))))
mutJ
as.dist(mutC)
```
Jaccard distance is a measure of how dissimilar two sets of data are; it is the complement of the Jaccard Index (i.e., how similar two sets of data are).  For example, in the mutj distance matrix above, strain 1 and 2 are ~ 80% different.    

Correlation-based distances are commonly used in gene expression analyses to identify clusters of observations with the same overall profiles.
  - Distance between two objects will be zero if they are perfectly correlated.


## *k*-methods

Useful for identifying spherial-shaped clusters that are compact, similar in size and well separated.

*k*-medoid
- Center of a given cluster is the most centrally located point in a cluster/ group (i.e., medoid)
- Robust
- Less sensitive to noise and outliers
- Need to specificy *k* (i.e., the number of clusters to be generated)

PAM Algorithm (Partioning Around Medoids)
 1. Select *k* clusters
 2. Randomly select *k* objects to become the medoids
 3. Assign each observation in the dataset to the nearest medoid to construct clusters
 4. Medoids are swapped with non-medoids
    - Distances are recalulated
 5. Repeat Step 3-4 until clusters stabilize

k-means
- Center of a given cluster is calculated as the mean value of all the data points in the cluster

How it works...
  1. Select *k* clusters
  2. Randomly select *k* objects to be the cluster centroids
  4. Assign items to the k clusters that contain the closest centroid
  5. Re-calculate the centroid for each of the k clusters (i.e., mean value of all the data points in the cluster)
        - Want the centroid to be as small as possible
  6. Calculate the distance of all items to the k centroids (i.e., Euclidean distance)
  7. Repeat until clusters assignments are stable (or the max # of iterations has been met)

**Question 5.3**
How does the *k*-means algorithm differ from the EM algorithm?

In the EM algorithm, each point participates in the computation of the mean of all groups through a probabilistic weight, while the points are either attributed to a cluster or not in the *k*-means algorithm, so each point participates only in the computation of the center of one cluster.

**Question 5.4**
Follow the vignette of the package *clusterExperiment* to generate Figure 5.10

Each column in the heatmap generated by the r chunk below corresponds to a cell and the colors represent the cluster assignments.

```{r}
options(getClass.msg = FALSE)
data("fluidigm")

fluidigm$sample_id.x  # 130 cells in the dataset

se = fluidigm[, fluidigm$Coverage_Type == "High"]
assays(se) = list(normalized_counts = 
   round(limma::normalizeQuantiles(assay(se))))
ce = clusterMany(se, clusterFunction = "pam", ks = 5:10, run = TRUE,
  isCount = TRUE, reduceMethod = "var", nFilterDims = c(60, 100, 150))
ce
clusterLabels(ce) = sub("FilterDims", "", clusterLabels(ce))
plotClusters(ce, whichClusters = "workflow", axisLine = -1)
```

The cells corresponding to the Green column are most stable (i.e., tight clusters with resampling); the variable genes for these cells are almost always clustering together.  In contrast, more variability in clustering is observed for the middle and right most columns.   


## Flow Cytometry and Mass Cytometry

What is Flow Cytometry?
https://www.antibodies-online.com/resources/17/1247/what-is-flow-cytometry-facs-analysis/

- Used to profile a population of cells
    - Fluorescent tags can be used to distinguish specific sub-populations within a larger group
    - Can be used to identify, separate, and characterize various immune cell subtypes based on size and morphology


Mass Cytometry
- Variation of flow cytometry 
- Uses antibodies labeled with heavy metal isotopes rather than fluorochromes
- Overcomes limitations in spectral overlap (i.e., flurochromes have broad emission spectra)
- Can include more markers

Let's look at some Mass Cytometry Data
```{r}
fcsB = read.FCS("/MBIO7160/ModernStatsforModernBiol/RData/Bendall_2011.fcs")
slotNames(fcsB)

fcsB
```

**Question 5.5**
(a) How many variables were measured in the fcsB object?

Based on the code chunk below, 41 variables were measured in fcsB, which includes 38 heavy metal isotopes.

```{r}
colnames(fcsB)
```

(b) Subset the data to look at the first few rows.  How many cells were measured?

Based on the R code chunk below, the results for 4 cells are displayed as well as the total number of cells measured, which is 91392.

```{r}
head(fcsB, n=4)

# Alternative using the exprs function
exprs(fcsB)[1:4,]

#Display all rows
exprs(fcsB)

```

## Data Preprocessing

- Preprocessing of the data is important for generating meaningful results

In the R code chunk below, the isotope names in fcsB are first replaced with the marker names and then plotted

```{r}
markersB = readr::read_csv("/MBIO7160/ModernStatsforModernBiol/RData/Bendall_2011_markers.csv")

#Show isotope and corresponding marker data
markersB

#Rename isotope with marker
mt = match(markersB$isotope, colnames(fcsB))
stopifnot(!any(is.na(mt)))
colnames(fcsB)[mt] = markersB$marker

#plot
flowPlot(fcsB, plotParameters = colnames(fcsB)[2:3], logy = TRUE)
```
Plot shows the presence of two cell populations identified by the DNA191 marker; each cluster consists of cells of similar lengths.


**Data Transformation: hyperbolic arcsine**

It is standard to transform both flow and mass cytometry data 
- Improve cell clustering
- Aids in data visualization 

```{r}
v1 = seq(0, 1, length.out = 100)
plot(log(v1), asinh(v1), type = 'l')
plot(v1, asinh(v1), type = 'l')
v3 = seq(30, 3000, length = 100)
plot(log(v3), asinh(v3), type= 'l')
```
Based on the plots generated above, it is apparent that smaller values have a linear relationship with the hyperbolic arcsine (asinh), while the log of larger values have a lineral relationship with asinh.

Another example of variance stabilizing transformation...

```{r}
asinhtrsf = arcsinhTransform(a = 0.1, b = 1)
fcsBT = transform(fcsB,
  transformList(colnames(fcsB)[-c(1, 2, 41)], asinhtrsf))

# Simple one-dimensional histogram before transformation
densityplot( ~`CD3all`, fcsB)

# After transformation; reveals bimodality and the existence of two cell populations
densityplot( ~`CD3all`, fcsBT)
```
Prior to transformation, the CD3all cells appear to consist of a single population with many zeros (i.e., zero inflated).  Following transformation, a bimodal distribution is observed indicating the existence of two cell populations.


**Question 5.6**
How many dimensions does the following code use to split the data into two groups?

Based on the functionality of kmeansFilter, which is a one dimensional k-means filter that takes a parameter agruement associated with two or more populations, a single dimension has been used to split the data into two groups.

```{r}
kf = kmeansFilter("CD3all" = c("Pop1","Pop2"), filterId="myKmFilter")

# What does kmeansFilter do?
?kmeansFilter

#Filter data
fres = flowCore::filter(fcsBT, kf)
fres

summary(fres)

fcsBT1 = flowCore::split(fcsBT, fres, population = "Pop1")
fcsBT1

fcsBT2 = flowCore::split(fcsBT, fres, population = "Pop2")
```

R code used to generate Figure 5.13

```{r}
library("flowPeaks")
fp = flowPeaks(Biobase::exprs(fcsBT)[, c("CD3all", "CD56")])
plot(fp)
```
After transformation, the calls were clustered using kmeans

To prevent overplotting, contours and shading can be used

```{r}
flowPlot(fcsBT, plotParameters = c("CD3all", "CD56"), logy = FALSE)
contour(fcsBT[, c(40, 19)], add = TRUE)
```

Use ggcyto to plot each patient in a different facet and compare to the results above.

```{r}
ggcd4cd8=ggcyto(fcsB,aes(x=CD4,y=CD8))
ggcd4=ggcyto(fcsB,aes(x=CD4))
ggcd8=ggcyto(fcsB,aes(x=CD8))
p1=ggcd4+geom_histogram(bins=60)
p1b=ggcd8+geom_histogram(bins=60)
asinhT = arcsinhTransform(a=0,b=1)
transl = transformList(colnames(fcsB)[-c(1,2,41)], asinhT)
fcsBT = transform(fcsB, transl)

p1t=ggcyto(fcsBT,aes(x=CD4))+geom_histogram(bins=90) 
p1t

p2t=ggcyto(fcsBT,aes(x=CD4,y=CD8))+geom_density2d(colour="black")
p2t

p3t=ggcyto(fcsBT,aes(x=CD45RA,y=CD20))+geom_density2d(colour="black")
p3t

```
The outputs from ggcyto and flowplot are similar, however, the flowplot output is much more informative.


## Density-based Clustering

- Clusters correspond to dense regions in the data space, separated by regions of lower density of points
- Does not require prior knowledge of *k*
- Can find any shape of clusters (i.e., the cluster doesn’t have to be circular)
- Can handle outliers
- Limitation = does not do well with clusters of similar density

DBSCAN (Density-based spatial clustering of applications with noise) 
- Works by dividing the data into *n* dimensions
  - Based on the notion of “clusters” and “noise”
- Start at a random point and count how many other points are nearby based on 2 parameters; *eps* and *minpts*
  - epsilon (“eps”)
    - How close points should be to each other to be considered a part of a cluster 
    - Points are considered neighbours if the distance between two points is lower or equal to this value 
    - DBSCAN is sensitive to the choice of eps
  - minimum points (“MinPts”)
    - Minimum number of points to form a dense region
    - Large dataset = large *minpts*
    - Can't be smaller than 3
- Continue the process until no other data points are nearby, and then it will look to form a second cluster.

DBSCAN Example

```{r}
library("dbscan")
mc5 = Biobase::exprs(fcsBT)[, c(15,16,19,40,33)]
res5 = dbscan::dbscan(mc5, eps = 0.65, minPts = 30)
mc5df = data.frame(mc5, cluster = as.factor(res5$cluster))
table(mc5df$cluster)
ggplot(mc5df, aes(x=CD4,    y=CD8,  col=cluster))+geom_density2d()
ggplot(mc5df, aes(x=CD3all, y=CD20, col=cluster))+geom_density2d()
```
Results of clustering with DBSCAN using 5 markers for the CD4-CD8 and CD3all-CD20 planes.


**Question 5.7**
Increase the dimension to 6

```{r}
mc6 = Biobase::exprs(fcsBT)[, c(15, 16, 19, 33, 25, 40)]
#Decrease minPts
res = dbscan::dbscan(mc6, eps = 0.65, minPts = 20)
mc6df = data.frame(mc6, cluster = as.factor(res$cluster))
table(mc6df$cluster)

#plot
ggplot(mc6df, aes(x=CD4,    y=CD8,  col=cluster))+geom_density2d()
ggplot(mc6df, aes(x=CD3all, y=CD20, col=cluster))+geom_density2d()
```
Results of clustering with DBSCAN using 6 markers for the CD4-CD8 and CD3all-CD20 planes.


Vary *eps* so that there are 4 clusters, where at least 2 of them have more than 100 points
```{r}
mc6 = Biobase::exprs(fcsBT)[, c(15, 16, 19, 33, 25, 40)]
#Increase eps to 0.95
res = dbscan::dbscan(mc6, eps = 0.95, minPts = 20)
mc6df = data.frame(mc6, cluster = as.factor(res$cluster))
table(mc6df$cluster)

#plot
ggplot(mc6df, aes(x=CD4,    y=CD8,  col=cluster))+geom_density2d()
ggplot(mc6df, aes(x=CD3all, y=CD20, col=cluster))+geom_density2d()
```
Clusters 1, 2, 3, 4, and 6 have more than 100 points.

Increase dimension to 7

```{r}
mc7 = Biobase::exprs(fcsBT)[, c(11, 15, 16, 19, 25, 33, 40)]
res = dbscan::dbscan(mc7, eps = 0.95, minPts = 20)
mc7df = data.frame(mc7, cluster = as.factor(res$cluster))
table(mc7df$cluster)

#plot
ggplot(mc7df, aes(x=CD4,    y=CD8,  col=cluster))+geom_density2d()
ggplot(mc7df, aes(x=CD3all, y=CD20, col=cluster))+geom_density2d()
```
As the dimensionality is increased to 7, the eps also has to increase in order to detect large enough clusters.


## Hierarchical Clustering

- Bottom-up approach
- Commonly visualized in a dendrogram (i.e., a tree like diagram)
- Do not need to know *k*
- Agglomerative or Divisive
- Agglomerative = initally each data point is considered as an individual cluster 
  - At each iteration, similar clusters merge with other clusters until one cluster or K clusters are formed
  
Once the clusters are aggregated, how do we calculate the distance between them to form the tree?

Single Linkage
  - Minimal Jump
    - Minimum distance between the closest two points from each cluster
  - Comb-like tree
Complete Linkage
  - Maximal Jump
    - Maximum distance between two points from different clusters
Average Linkage
  - Computes the average similarity for all the pairs of points
  - Useful if there is a lot of noise separating clusters
Ward's Method
  - Analysis of variance approach
  - Maximizes the between-groups sum of squares
  - Minimizes the within-groups sum of squares
  
When prior knowledge of cluster size is known (i.e., they are known to be similar in size), average linkage or Ward's method is considered to be the best tactic, however, this is often not something that is known.

**Question 5.8**

Hierarchical Clustering for Cell Populations

Example: Gene expression measurements for 156 genes on T cells of 3 types (Naive, Effector and Memory)

Heatmaps can be used to visualize large, matrix-like datasets; provides an illustration of the patterns that might be contained within the data

```{r}
load("/MBIO7160/ModernStatsforModernBiol/RData/Morder.RData")
Morder

#What does the data contain
colnames(Morder)
rownames(Morder)

#How does heatmap work?
?pheatmap

#Euclidean only
pheatmap(as.matrix(dist(Morder), "euclidean"), 
         cluster_rows=FALSE, cluster_cols=FALSE, cellwidth=10, cellheight=10)


#Manhattan only
pheatmap(as.matrix(dist(Morder), "manhattan"), 
         cluster_rows=FALSE, cluster_cols=FALSE, cellwidth=10, cellheight=10)

```
**Question 5.9**

```{r}
#Plot with dendrogram

#Euclidean only
#Turn data into distance matrix
pheatmap(as.matrix(dist(Morder), "euclidean"), 
         cluster_rows=FALSE, cluster_cols=TRUE, cellwidth=10, cellheight=10)


#Manhattan only
#Turn data into distance matrix
pheatmap(as.matrix(dist(Morder), "manhattan"), 
         cluster_rows=FALSE, cluster_cols=TRUE, cellwidth=10, cellheight=10)

```

Add some commentary... 

**Question 5.10**

There are at least 9 different ways to order the tip labels in Figure 5.22.

Is there a type of permutation that can be used to deteremine the true answer?

## Validating and Choosing the # of Clusters

Different criteria that can be used to assess the quality of clustering

WSS
- Maximize the between-groups differences while maintaining a small within-group distance
- Within-group sum of squares
- Can be used as a function of *k*
  - Decreasing function
  - Pronounced region of a steep decrease and then plateau (i.e., elbow) considered a potential "sweet spot" for the number of clusters
    - Adding another cluster does not improve the WSS by much

When attempting to decide how many clusters are appropriate for the data, it can be useful to look at cases where the answer is actually known.

Example
Simulate data from four groups...

```{r}
library("dplyr")
set.seed(1234)
simdat = lapply(c(0, 8), function(mx) {
  lapply(c(0,8), function(my) {
    tibble(x = rnorm(100, mean = mx, sd = 2),
           y = rnorm(100, mean = my, sd = 2),
           class = paste(mx, my, sep = ":"))
   }) %>% bind_rows
}) %>% bind_rows
simdat

simdatxy = simdat[, c("x", "y")]
```

Plot the data and observe the four clusters
```{r}
ggplot(simdat, aes(x = x, y = y, col = class)) + geom_point() +
  coord_fixed()
```
Compute the within-groups sum of squares for the clusters obtained from the k-means method

```{r}
wss = tibble(k = 1:8, value = NA_real_)
wss$value[1] = sum(scale(simdatxy, scale = FALSE)^2)
for (i in 2:nrow(wss)) {
  km  = kmeans(simdatxy, centers = wss$k[i])
  wss$value[i] = sum(km$withinss)
}
wss
ggplot(wss, aes(x = k, y = value)) + geom_col()
```
From the plot, k=4 looks like the optimal cluster number


**Question 5.12**
Run the code above and compare the wss values.  Why are the values different?

(a) The values differ slightly between runs because the process is continually repeated; some datapoints may be assigned to a different cluster, resulting in slightly different results

Really good resource explaining k-means
https://towardsdatascience.com/k-means-clustering-from-a-to-z-f6242a314e9a

(b) Compute the WSS value for uniform data.

```{r}
#Replace rnorm with runif and maintain a similar range of values...
set.seed(1234)
simdat2 = lapply(c(0, 8), function(mx) {
  lapply(c(0,8), function(my) {
    tibble(x = runif(100, min = -5, max = 13),
           y = runif(100, min = -5, max = 13),
           class = paste(mx, my, sep = ":"))
   }) %>% bind_rows
}) %>% bind_rows
simdat2
simdatxy2 = simdat2[, c("x", "y")]

#plot the data
wss2 = tibble(k = 1:8, value = NA_real_)
wss2$value[1] = sum(scale(simdatxy2, scale = FALSE)^2)
for (i in 2:nrow(wss2)) {
  km  = kmeans(simdatxy2, centers = wss2$k[i])
  wss2$value[i] = sum(km$withinss)
}
wss2
ggplot(wss2, aes(x = k, y = value)) + geom_col()
```

k=4 still appears to be the best choice for the number of clusters for the same range and dimensions.

##Calinski-Harabasz

Calculates the ratio of the BSS (between groups sum of squares) and WSS variances for different choices of *k*

```{r}
library("fpc")
library("cluster")
CH = tibble(
  k = 2:8,
  value = sapply(k, function(i) {
    p = pam(simdatxy, i)
    calinhara(simdatxy, p$cluster)
  })
)
ggplot(CH, aes(x = k, y = value)) + geom_line() + geom_point() +
  ylab("CH index")
```
k=4 still appears to be the best cluster choice (i.e., elbow)


##Gap Statistic

- Used to determine the optimal number of clusters; common algorithm used for cluster validation.

- Compares the log of the WSS to the averages from simulated data with less structure

Optimal number of clusters (i.e., K) is the value that yields the largest gap statistic

**Question 5.14**
Make a function that plots the gap statistic and show the output for simdat

```{r}
pamfun = function(x, k)
  list(cluster = pam(x, k, cluster.only = TRUE))

gss = clusGap(simdatxy, FUN = pamfun, K.max = 8, B = 50,
              verbose = FALSE)
plot_gap = function(x) {
  gstab = data.frame(x$Tab, k = seq_len(nrow(x$Tab)))
  ggplot(gstab, aes(k, gap)) + geom_line() +
    geom_errorbar(aes(ymax = gap + SE.sim,
                      ymin = gap - SE.sim), width=0.1) +
    geom_point(size = 3, col=  "red")
}
plot_gap(gss)
```
Highest Gap Statistic is at k=4


Example with real data

Load the data...
```{r}
data("x")
x
```
Choose the 50 most variable genes...

```{r}
selFeats = order(rowVars(Biobase::exprs(x)), decreasing = TRUE)[1:50]
selFeats
set.seed(1234)
embmat = t(Biobase::exprs(x)[selFeats, ])
embgap = clusGap(embmat, FUN = pamfun, K.max = 24, verbose = FALSE)
k1 = maxSE(embgap$Tab[, "gap"], embgap$Tab[, "SE.sim"])
k2 = maxSE(embgap$Tab[, "gap"], embgap$Tab[, "SE.sim"],
           method = "Tibs2001SEmax")
c(k1, k2)
```
Above code chunk reports a cluster size of *k* = 9 (i.e., gap is not larger than the first local maximum minus a standard error) and *k* = 7 (choice recommended by Tibshirani et al., 2001)

View manual page for clusGap

```{r}
?clusGap
```

Plot the gap statistic

```{r}
plot(embgap, main = "")
cl = pamfun(embmat, k = k1)$cluster
table(pData(x)[names(cl), "sampleGroup"], cl)
```
At around k=9 there is less of a change in the gap statistic as the number of clusters increases (i.e., stabilizes).

**Question 5.15**
How do the results change if you use all the features in x?

It takes a VERY LONG time to run... and causes R Studio to freeze

```{r}#
selFeats2 = order(rowVars(Biobase::exprs(x)), decreasing = TRUE)
selFeats2
embmat2 = t(Biobase::exprs(x)[selFeats2, ])
embgap2 = clusGap(embmat2, FUN = pamfun, K.max = 24, verbose = FALSE)
k3 = maxSE(embgap2$Tab[, "gap"], embgap2$Tab[, "SE.sim"])
k4 = maxSE(embgap2$Tab[, "gap"], embgap2$Tab[, "SE.sim"],
           method = "Tibs2001SEmax")
c(k3, k4)


#Plot
plot(embgap2, main = "")
cl = pamfun(embmat2, k = k3)$cluster
table(pData(x)[names(cl), "sampleGroup"], cl)
```

##Cluster Validation using the Bootstrap

- Check for cluster stability over time

Use the Hiiragi2013 data to test the following hypothesis:

The inner cell mass (ICM) of the mouse blastocyst on embryonic day 3.5 (E3.5) falls into two clusters corresponding to pluripotent epiblast (EPI) vs. primitive endoderm (PE) and embryonic day 3.25 (E3.25) does not demonstrate this.

```{r}
clusterResampling = function(x, ngenes = 50, k = 2, B = 250,
                             prob = 0.67) {
  mat = Biobase::exprs(x)
  ce = cl_ensemble(list = lapply(seq_len(B), function(b) {
    selSamps = sample(ncol(mat), size = round(prob * ncol(mat)),
                      replace = FALSE)
    submat = mat[, selSamps, drop = FALSE]
    sel = order(rowVars(submat), decreasing = TRUE)[seq_len(ngenes)]
    submat = submat[sel,, drop = FALSE]
    pamres = pam(t(submat), k = k)
    pred = cl_predict(pamres, t(mat[sel, ]), "memberships")
    as.cl_partition(pred)
  }))
  cons = cl_consensus(ce)
  ag = sapply(ce, cl_agreement, y = cons)
  list(agreements = ag, consensus = cons)
}
```
A good explaination of the code chunk above is on pg. 128

Calculate the Euclidean dissimilarity

```{r}
iswt = (x$genotype == "WT")
cr1 = clusterResampling(x[, x$Embryonic.day == "E3.25" & iswt])
cr2 = clusterResampling(x[, x$Embryonic.day == "E3.5"  & iswt])
```
Create a tibble

```{r}
ag1 = tibble(agreements = cr1$agreements, day = "E3.25")
ag2 = tibble(agreements = cr2$agreements, day = "E3.5")

#Beeswarm
ggplot(bind_rows(ag1, ag2), aes(x = day, y = agreements)) +
  geom_boxplot() +
  ggbeeswarm::geom_beeswarm(cex = 1.5, col = "#0000ff40")
```
Based on the plot created above, 1 indicates perfect agreement and lower values indicate less agreement.  Thus, there is a high agreement that the mouse blastocyst on embryonic day 3.5 (E3.5) falls into two clusters corresponding to pluripotent epiblast (EPI) vs. primitive endoderm (PE).

Plot the Membership probabilities of the consensus clustering 

```{r}
#Facet
mem1 = tibble(y = sort(cl_membership(cr1$consensus)[, 1]),
              x = seq(along = y), day = "E3.25")
mem2 = tibble(y = sort(cl_membership(cr2$consensus)[, 1]),
              x = seq(along = y), day = "E3.5")
ggplot(bind_rows(mem1, mem2), aes(x = x, y = y, col = day)) +
  geom_point() + facet_grid(~ day, scales = "free_x")
```

For E3.25, the probabilities are diffuse indicating that the individual clusterings have low agreement, while the distribution is bimodal for E3.5 with only one ambiguous sample.  Further supports the hypothesis that the mouse blastocyst on embryonic day 3.5 (E3.5) falls into two clusters.

##Clustering as a Means for Denoising

Example of noisy observations with different baselines frequencies

seq1 = 10^3
seq2 = 10^5

```{r}
seq1 = rmvnorm(n = 1e3, mu = -c(1, 1), sigma = 0.5 * diag(c(1, 1)))
seq2 = rmvnorm(n = 1e5, mu =  c(1, 1), sigma = 0.5 * diag(c(1, 1)))
twogr = data.frame(
  rbind(seq1, seq2),
  seq = factor(c(rep(1, nrow(seq1)),
                 rep(2, nrow(seq2))))
)

colnames(twogr)[1:2] = c("x", "y")

ggplot(twogr, aes(x = x, y = y, colour = seq, fill = seq)) +
  geom_hex(alpha = 0.5, bins = 50) + coord_fixed()
```

Although both groups have noise distributions with similar variances, the radii of the groups are very different.  There are many more opportunities for errors in seq2 than in seq1.  Therefore, frequencies are important in clustering the data.


**Question 5.16**
Cluster the two groups according to distance from the group center

```{r}
#Need to use CLARA (Clustering Large Applications); PAM function won't run for large datasets (i.e., more than 65536)

twogr2 = clara(twogr, k = 2, metric = c("euclidean"), pamLike = TRUE, samples = 50)

#Plot CLARA results using fviz_cluster
fviz_cluster(twogr2, geom = c("point"), show.clust.cent = TRUE)

#Combine CLARA results with original dataset
combine = cbind(twogr, cluster = twogr2$clustering)
combine

#Plot combined data using ggplot
ggplot(combine, aes(x = x, y = y, group = factor(cluster), colour = factor(cluster))) + 
  geom_point() + coord_fixed() + labs(color = "Cluster")

```

16S rRNA Read Clustering Example

Generate Figure 5.13: The diameter of a set of sequences as a function of the number of sequences

```{r}
#Simulate n=2000 binary variables of length 200 and a sequencing errors = 0.001

n    = 2000
len  = 200
perr = 0.001
seqs = matrix(runif(n * len) >= perr, nrow = n, ncol = len)

seqs

#Compute pairwise distances

dists = as.matrix(dist(seqs, method = "manhattan"))

#Maximum distance within a set of reads

dfseqs = tibble(
  k = 10 ^ seq(log10(2), log10(n), length.out = 20),
  diameter = vapply(k, function(i) {
    s = sample(n, i)
    max(dists[s, s])
    }, numeric(1)))
ggplot(dfseqs, aes(x = k, y = diameter)) + geom_point()+geom_smooth()
```
As the number of sequences increases, the pairwise distance between the reads also decreases.

## De-noising 16S rRNA Sequences

What is 16S rRNA?
- Variable region in bacteria that can be used for identification

FastQ File
- Contains the raw sequence data (i.e., A, T, G, and C)
- Each base call has a quality score associated with it (i.e., how confident can we be in the particular base call?)
- Example = A Q-Score of 30 indicates a base calling accuracy of 99.9% (1 in 1000 chance the base is wrong)

ASVs
- Amplicon Sequence Variant
- DNA sequences recovered from a high-throughput marker gene analysis following the removal of spurious sequences  
- Considered to be inferred sequences of true biological origin
- Provides enhanced resolution (i.e., single nucleotide difference)

**Question 5.17**
What is the distribution for two sequences of length 200 with technlogical sequencing errors occuring at 0.0005.

Use Monte Carlo Simulation...

```{r}
simseq10K = replicate(1e5, sum(rpois(200, 0.0005)))
mean(simseq10K)
vcd::distplot(simseq10K, "poisson")
```
It is Poisson(0.1); n = 200, p = 0.0005, so n*p = 0.1


## Inferring Sequence Variants

dada
- Divisive Amplicon Denoising Algorithm
  - Distinguishes sequencing errors from real biological variation
  - Model parameters are estimated from the data using an EM-type approach

Read in the data and estimate the errors
```{r}
derepFs = readRDS(file="/MBIO7160/ModernStatsforModernBiol/RData/derepFs.rds")
derepRs = readRDS(file="/MBIO7160/ModernStatsforModernBiol/RData/derepRs.rds")

ddF = dada(derepFs, err = NULL, selfConsist = TRUE)
ddR = dada(derepRs, err = NULL, selfConsist = TRUE)
```

Verify the error transition rates have been well estimated by plotting the frequencies of each type of nucleotide transition as a function of quality.

```{r}
#Plot errors
plotErrors(ddF)
plotErrors(ddR)

```
As the base quality increases (i.e., consensus quality score), the error frequency also decreases (with the exception of A2A, C2C, G2G, and T2T).

Re-run algorithm to find sequence variants.

```{r}
dadaFs = dada(derepFs, err=ddF[[1]]$err_out, pool = TRUE)
dadaRs = dada(derepRs, err=ddR[[1]]$err_out, pool = TRUE)
```

Merge the inferred forward and reverse sequence.

```{r}
mergers = mergePairs(dadaFs, derepFs, dadaRs, derepRs)
```

Generate a contingency table of counts of ASVs.

```{r}
seqtab.all = makeSequenceTable(mergers[!grepl("Mock",names(mergers))])
```

**Question 5.18**

Explore the components of dadaRs and mergers

```{r}
length(dadaRs) 
names(dadaRs)
dadaRs$F3D0
dadaRs$F3D1

length(mergers) 
names(mergers)
```

Chimeras
- Artifact sequences formed by two or more biological sequences incorrectly joined together.

Remove Chimeras from the sequence data

```{r}
seqtab = removeBimeraDenovo(seqtab.all)

#What does the function do?
?removeBimeraDenovo
```

**Question 5.19**
(a) Why are chimera easy to remove?

removeBimeraDenovo function looks for an exact bimera (i.e., two parent chimera where the left side consists of one parent sequene and the right side is made up of a second parent sequence) of the parent sequences that matches the input sequence.

Therefore, Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences.

(b) What proportion of reads were chimeric in seqtab.all data?

```{r}
Before = length(seqtab.all)

After = length(seqtab)

Total = (Before - After) / (Before) 
Total

```
2812 Chimeric sequences were removed = ~35%

(c) What proportion of unique sequence variants are chimeric?

```{r}
UniqB = sum(unique(seqtab.all))
UniqB

UniqA = sum(unique(seqtab))
UniqA

TotalUni = (UniqB - UniqA) / (UniqB)
TotalUni
```

7% of unique sequence variants were chimeric.

**Exercises**

**5.1**

(a) Compute the silhouette index for simdat

The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then number of clusters is appropriate.

```{r}
pam4 = pam(simdatxy, 4)
sil4 = silhouette(pam4, 4)
plot(sil4, col=c("red","green","blue","purple"), main="Silhouette")
```
k = 4, Avg Silhouette = 0.53


(b) Change the number of clusters *k*

```{r}
# k = 5, Avg Silhouette = 0.48
pam5 = pam(simdatxy, 5)
sil5 = silhouette(pam5, 5)
plot(sil5, col=c("red","green","blue","purple"), main="Silhouette")

# k = 6 , Avg Silhouette = 0.44
pam6 = pam(simdatxy, 6)
sil6 = silhouette(pam6, 6)
plot(sil6, col=c("red","green","blue","purple"), main="Silhouette")

# k = 2, Avg Silhouette = 0.32
pam2 = pam(simdatxy, 2)
sil2 = silhouette(pam2, 2)
plot(sil2, col=c("red","green","blue","purple"), main="Silhouette")

# k = 11, Avg Silhouette = 0.43

pam11 = pam(simdatxy, 11)
sil11 = silhouette(pam11, 11)
plot(sil11, col=c("red","green","blue","purple"), main="Silhouette")
```
Based on the results above, it appears k = 4 gives the best results.


(c) Repeat for groups with a uniform distribution over a range of values.

```{r}
#Use the same simdatxy2 as in Question 5.12
simdatxy2

pam2 = pam(simdatxy2, 2)
sil2 = silhouette(pam2, 2)
plot(sil2, col=c("red","green","blue","purple"), main="Silhouette")
#Avg Sil = 0.38

pam3 = pam(simdatxy2, 3)
sil3 = silhouette(pam3, 3)
plot(sil3, col=c("red","green","blue","purple"), main="Silhouette")
#Avg Sil = 0.39

pam4 = pam(simdatxy2, 4)
sil4 = silhouette(pam4, 4)
plot(sil4, col=c("red","green","blue","purple"), main="Silhouette")
#Avg Sil = 0.4

pam5 = pam(simdatxy2, 5)
sil5 = silhouette(pam5, 5)
plot(sil5, col=c("red","green","blue","purple"), main="Silhouette")
#Avg Sil = 0.36

pam6 = pam(simdatxy2, 6)
sil6 = silhouette(pam6, 6)
plot(sil6, col=c("red","green","blue","purple"), main="Silhouette")
#Avg Sil = 0.37

pam7 = pam(simdatxy2, 7)
sil7 = silhouette(pam7, 7)
plot(sil7, col=c("red","green","blue","purple"), main="Silhouette")
#Avg Sil = 0.37

pam8 = pam(simdatxy2, 8)
sil8 = silhouette(pam8, 8)
plot(sil8, col=c("red","green","blue","purple"), main="Silhouette")
#Avg Sil = 0.37
```
Difficult to get Avgerage Silhouette width close to 1...

Based on the simulated data above, k=4 appears to give the best result (Avg Sil width = 0.4)

**Exercise 5.2**

(a) Make a character representation of the distance between the 20 locations in the dune data from the vegan package using the function *symnum*

symnum
- Used to visualize structured matrices (i.e., symbolically encoded numeric or logical array).

```{r}
library(vegan)
data("dune")

dune

#Transform dune data into correlation matrix
corMat = cor(dune)
corMat

#symnum = Symbolic Number Coding
?symnum

#Run symnum
symnum(corMat, abbr.colnames = FALSE)
```
Hyporadi and Empenigr are highly correlated with respect to location.

(b) Make a heatmap of these distances

```{r}
#heatmap
pheatmap(corMat, cluster_rows = FALSE, cluster_cols = FALSE, color = inferno(20))
```

**5.3**

Spiral Graph generated using kmeans

(a) Plot 
```{r}
data("spirals")
clusts = kmeans(spirals,2)$cluster
plot(spirals, col = c("blue", "red")[clusts])
```
kmeans is not ideal for non-convex clusters; we only get two clusters.


(b) Repeat using dbscan

```{r}
data("spirals", package = "kernlab")
res.dbscan = dbscan::dbscan(spirals, eps = 0.16, minPts = 3)
plot(spirals,col=c("blue","red","forestgreen")[res.dbscan$cluster])
```
DBSCAN is much better at handling non-convex clusters; we get 3 clusters in this case.


(c) Repeat with different parameters

```{r}
#Modify minPts
res.dbscan = dbscan::dbscan(spirals, eps = 0.16, minPts = 4)
plot(spirals,col=c("blue","red","forestgreen")[res.dbscan$cluster])

res.dbscan = dbscan::dbscan(spirals, eps = 0.16, minPts = 5)
plot(spirals,col=c("blue","red","forestgreen")[res.dbscan$cluster])

res.dbscan = dbscan::dbscan(spirals, eps = 0.16, minPts = 6)
plot(spirals,col=c("blue","red","forestgreen")[res.dbscan$cluster])

res.dbscan = dbscan::dbscan(spirals, eps = 0.16, minPts = 7)
plot(spirals,col=c("blue","red","forestgreen")[res.dbscan$cluster])

res.dbscan = dbscan::dbscan(spirals, eps = 0.16, minPts = 8)
plot(spirals,col=c("blue","red","forestgreen")[res.dbscan$cluster])


#Modify eps
res.dbscan = dbscan::dbscan(spirals, eps = 0.17, minPts = 4)
plot(spirals,col=c("blue","red","forestgreen")[res.dbscan$cluster])

res.dbscan = dbscan::dbscan(spirals, eps = 0.18, minPts = 5)
plot(spirals,col=c("blue","red","forestgreen")[res.dbscan$cluster])

res.dbscan = dbscan::dbscan(spirals, eps = 0.19, minPts = 6)
plot(spirals,col=c("blue","red","forestgreen")[res.dbscan$cluster])

res.dbscan = dbscan::dbscan(spirals, eps = 0.20, minPts = 7)
plot(spirals,col=c("blue","red","forestgreen")[res.dbscan$cluster])

res.dbscan = dbscan::dbscan(spirals, eps = 0.21, minPts = 8)
plot(spirals,col=c("blue","red","forestgreen")[res.dbscan$cluster])
```
The number of groupings is not robust; changes frequently depending on the parameters.

**5.4**

https://www.huffpost.com/entry/post_b_817254

What are some reasons for the clustering and high incidence rates on the West and East Coasts and around Chicago?

Underreporting?  
Geographical distribution of susceptible populations?
Life-Style?


**5.5**
Amplicon Bioinformatics

```{r}
base_dir = "/MBIO7160/ModernStatsforModernBiol/RData/"
miseq_path = file.path(base_dir, "MiSeq_SOP")
filt_path = file.path(miseq_path, "filtered")
fnFs = sort(list.files(miseq_path, pattern="_R1_001.fastq"))
fnRs = sort(list.files(miseq_path, pattern="_R2_001.fastq"))
sampleNames = sapply(strsplit(fnFs, "_"), `[`, 1)
if (!file_test("-d", filt_path)) dir.create(filt_path)
filtFs = file.path(filt_path, paste0(sampleNames, "_F_filt.fastq.gz"))
filtRs = file.path(filt_path, paste0(sampleNames, "_R_filt.fastq.gz"))
fnFs = file.path(miseq_path, fnFs)
fnRs = file.path(miseq_path, fnRs)

print(length(fnFs))
```

Sequence Quality Plots

```{r}
plotQualityProfile(fnFs[1:2]) + ggtitle("Forward")
plotQualityProfile(fnRs[1:2]) + ggtitle("Reverse")
```
The lines show positional summary stats; green is the mean, organge is the median and the dashed organge lines are the 25th and 75th percentile.

**Exercise 5.6**
Generate similar plots for four randomly selected sets of forward and reverse reads.

```{r}
plotQualityProfile(fnFs[7:8]) + ggtitle("Forward")
plotQualityProfile(fnRs[7:8]) + ggtitle("Reverse")

plotQualityProfile(fnFs[9:10]) + ggtitle("Forward")
plotQualityProfile(fnRs[9:10]) + ggtitle("Reverse")

plotQualityProfile(fnFs[11:12]) + ggtitle("Forward")
plotQualityProfile(fnRs[11:12]) + ggtitle("Reverse")
```
Reverse reads tend to be of lower quality; this is largely attributed to the sequencing chemistry.

**5.7**
Trimming and Filtering

Always make sure the number of sequences/ reads is consistent between forward and reverse pairs.

```{r}

#Following the dada2 vignette

filtF8 <- tempfile(fileext=".fastq.gz")
filtR8 <- tempfile(fileext=".fastq.gz")

#Filter and trim
filterAndTrim(fwd=fnFs[8], filt=filtF8, rev=fnRs[8], filt.rev=filtR8,
                  trimLeft=10, truncLen=c(240, 160), 
                  maxN=0, maxEE=2)

#Plot Filtered Data

plotQualityProfile(filtF8) + ggtitle("Forward")
plotQualityProfile(filtR8) + ggtitle("Reverse")

#Compare to unfiltered data
plotQualityProfile(fnFs[8]) + ggtitle("Forward")
plotQualityProfile(fnRs[8]) + ggtitle("Reverse")

```
The filterAndTrim(...) function filters the forward and reverse reads jointly, outputting only those pairs of reads that both pass the filter. 

In this function call we did four things: We removed the first trimLeft=10 nucleotides of each read. We truncated the forward and reverse reads at 240, 160 nucleotides respectively. 

We filtered out all reads with more than maxN=0 ambiguous nucleotides, as well as all reads with more than two expected errors (maxEE=2).

*5.8*
Mystery Question only in the online textbook

...didn't want to pay for the data