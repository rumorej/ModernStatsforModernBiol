---
title: "Chapter5_Clustering"
author: "Jill R"
date: "10/21/2019"
output: html_document
---

```{r setup, include=FALSE}
library(here)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(readr)
library(vegan)
library("clusterExperiment")
library("flowCore")
library("scRNAseq")
library("clue")
library(cluster)
library("flowCore")
library("flowViz")
library(pheatmap)
library(RColorBrewer)
library(viridis)
library(mixtools)
library(dada2)

knitr::opts_chunk$set(echo = TRUE)
```
## Chapter Goals

1. How to compare observations based on distances that are scientifically meaningful
2. Explore two approaches to clustering
  - Iterative Partioning
    - k-means
    - k-medoids
  - Hierarchical Clustering
3. Explore density based clustering for lower dimernsional data
4. Cluster validation
5. Distances and probabilities
  - Important to consider baseline frequencies and local densities when clustering

## Clustering

Show groupings that are important for interpreting the data, which can lead to discoveries, for example, John Snow's map of the cholera cases.

Study how to find meaningful clusters or groups in both low-dimensional and high-dimensional *nonparametric* settings.
- Caveat
  - Clustering algorithms are designed to find clusters
  - Find clusters even where there are none
  - Cluster validation is essential
  
Useful technique for understanding multivariate data
- It is an unsupervised method

Types of Clustering Algorithms in this Chapter

Connectivity models
- Based on the notion that the data points closer in data space exhibit more similarity to each other than the data points lying farther away
- Easy to interpret 
- Lack scalability for handling big datasets
 - Example = Hierarchical Clustering
  - Deteremine the number of appropirate clusters by interpreting the dendrogram

Centroid models
- Iterative clustering algorithms in which the notion of similarity is derived by the closeness of a data point to the centroid of the clusters
- requires prior knowledge of K (i.e., the number of clusters you want to divide your data in to)
  - have to decide the number of clusters beforehand
  - K-Means clustering algorithm is a popular example
    - Works well for Big Data

Distribution models
- Based on the notion of how probable is it that all data points in the cluster belong to the same distribution (For example: Normal, Gaussian). - These models often suffer from overfitting
- Example of these models is Expectation-maximization algorithm 

Density Models
- Search the data space for areas of varied density of data points in the data space. 
- Isolates various different density regions and assign the data points within these regions in the same cluster. 
- Examples of density models are DBSCAN and OPTICS.


Not suprisingly, there are a plethora of clustering tools...

Website for clustering tools in BioConductor
https://www.bioconductor.org/packages/devel/BiocViews.html#___Clustering


How the distances are measured and similarities between observations have an impact on the clustering result.

Euclidean
Manhattan
Maximum
Weighted Euclidean
Minkowski
And many many more...


## Computations related to distances in R

Function below returns a special object of class dist, which computes one of six choices of distance (euclidean, maximum, manhattan, canberra, binary and minkowski)

```{r}
mx  = c(0, 0, 0, 1, 1, 1)
my  = c(1, 0, 1, 1, 0, 1)
mz  = c(1, 1, 1, 0, 1, 1)
mat = rbind(mx, my, mz)
dist(mat)
dist(mat, method = "binary")
```

To access the distance between observation 1 and 2, turn the dist class object back into the matrix

```{r}
load("/MBIO7160/ModernStatsforModernBiol/RData/Morder.RData")
Morder
sqrt(sum((Morder[1, ] - Morder[2, ])^2))
as.matrix(dist(Morder))[2, 1]
```
Compute the Jaccard distance

```{r}
mut = read.csv("/MBIO7160/ModernStatsforModernBiol/RData/HIVmutations.csv")
mut[1:3, 10:16]
```

**Question 5.2**
Compare the Jaccard distance between mutations in the HIV dat mut to the correlation-based distance

```{r}
library("vegan")
mutJ = vegdist(mut, "jaccard")
mutC = sqrt(2 * (1 - cor(t(mut))))
mutJ
as.dist(mutC)
```

## Nonparametric mixture detection

Medoids
- Centers of groups

PAM
- Partioning around medoids
 1. Start from a matrix of *p* features on a set on *n* observations
 2. Randomly pick *k* distinct *cluster centers* out of the *n* observations ("seeds)
 3. Assign each of the remaining observations to the group whose center it is the closest.
 4. For each group, choose a new center from the observations in the group, such that the sum of the distances of group members to the center is the minimal; this is called the mediod
 5. Repeat 3 and 4 until the groups stabilize.
 
**Question 5.3**
How dose the *k*-means algorithm differ from the EM algorithm?

In the EM algorithm, each point participates in the computation of the mean of all groups through a probabilistic weight, while the points are either attributed to a cluster or not in the *k*-means algorithm, so each point participates only in the computation of the center of one cluster.

**Question 5.4**
Follow the vignette of the package *clusterExperiment*

```{r}
library("clusterExperiment")

#options(getClass.msg = FALSE)
data("fluidigm")

set.seed(1234)
se = fluidigm[, fluidigm$Coverage_Type == "High"]
assays(se) = list(normalized_counts = 
   round(limma::normalizeQuantiles(assay(se))))
ce = clusterMany(se, clusterFunction = "pam", ks = 5:10, run = TRUE,
  isCount = TRUE, reduceMethod = "var", nFilterDims = c(60, 100, 150))
clusterLabels(ce) = sub("FilterDims", "", clusterLabels(ce))
plotClusters(ce, whichClusters = "workflow", axisLine = -1)
```

## Clustering examples

Flow Cytometry and Mass Cytometry

```{r}

fcsB = read.FCS("/MBIO7160/ModernStatsforModernBiol/RData/Bendall_2011.fcs")
slotNames(fcsB)

fcsB
```

**Question 5.5**
(a) How many variables were measured in the fcsB object?

```{r}
colnames(fcsB)
```
There are 41 variables in fcsB

(b) Subset the data to look at the first few rows.  How many cells were measured?

```{r}
head(fcsB, n=4)

exprs(fcsB)[1:4,]

```

## Data Preprocessing

```{r}
markersB = readr::read_csv("/MBIO7160/ModernStatsforModernBiol/RData/Bendall_2011_markers.csv")
mt = match(markersB$isotope, colnames(fcsB))
stopifnot(!any(is.na(mt)))
colnames(fcsB)[mt] = markersB$marker

#plot
flowPlot(fcsB, plotParameters = colnames(fcsB)[2:3], logy = TRUE)
```

**Data Transformation: hyperbolic arcsine**

```{r}
v1 = seq(0, 1, length.out = 100)
plot(log(v1), asinh(v1), type = 'l')
plot(v1, asinh(v1), type = 'l')
v3 = seq(30, 3000, length = 100)
plot(log(v3), asinh(v3), type= 'l')
#abline(0, 1, col = "Red")
```

```{r}
asinhtrsf = arcsinhTransform(a = 0.1, b = 1)
fcsBT = transform(fcsB,
  transformList(colnames(fcsB)[-c(1, 2, 41)], asinhtrsf))
densityplot( ~`CD3all`, fcsB)
densityplot( ~`CD3all`, fcsBT)
```

**Question 5.6**
How many dimensions does the following code use to split the data into two groups?

```{r}
kf = kmeansFilter("CD3all" = c("Pop1","Pop2"), filterId="myKmFilter")
fres = flowCore::filter(fcsBT, kf)
summary(fres)
fcsBT1 = flowCore::split(fcsBT, fres, population = "Pop1")
fcsBT2 = flowCore::split(fcsBT, fres, population = "Pop2")
```
Code used to generate Figure 5.13

```{r}
library("flowPeaks")
fp = flowPeaks(Biobase::exprs(fcsBT)[, c("CD3all", "CD56")])
plot(fp)
```

To prevent overplotting, use contours and shading produced by the following code

```{r}
flowPlot(fcsBT, plotParameters = c("CD3all", "CD56"), logy = FALSE)
contour(fcsBT[, c(40, 19)], add = TRUE)
```

Plot each patient in a different facet

Need to revisit this....

```{r}
library("ggcyto")
library("labeling")
ggcd4cd8=ggcyto(fcsB,aes(x=CD4,y=CD8))
ggcd4=ggcyto(fcsB,aes(x=CD4))
ggcd8=ggcyto(fcsB,aes(x=CD8))
p1=ggcd4+geom_histogram(bins=60)
p1b=ggcd8+geom_histogram(bins=60)
asinhT = arcsinhTransform(a=0,b=1)
transl = transformList(colnames(fcsB)[-c(1,2,41)], asinhT)
fcsBT = transform(fcsB, transl)

p1t=ggcyto(fcsBT,aes(x=CD4))+geom_histogram(bins=90) 
p1t

p2t=ggcyto(fcsBT,aes(x=CD4,y=CD8))+geom_density2d(colour="black")
p2t

p3t=ggcyto(fcsBT,aes(x=CD45RA,y=CD20))+geom_density2d(colour="black")
p3t

fcsBT



```

## Density-based Clustering

```{r}
library("dbscan")
mc5 = Biobase::exprs(fcsBT)[, c(15,16,19,40,33)]
res5 = dbscan::dbscan(mc5, eps = 0.65, minPts = 30)
mc5df = data.frame(mc5, cluster = as.factor(res5$cluster))
table(mc5df$cluster)
ggplot(mc5df, aes(x=CD4,    y=CD8,  col=cluster))+geom_density2d()
ggplot(mc5df, aes(x=CD3all, y=CD20, col=cluster))+geom_density2d()
```

**Question 5.7**
Increase the dimension to 6

```{r}
mc6 = Biobase::exprs(fcsBT)[, c(15, 16, 19, 33, 25, 40)]
res = dbscan::dbscan(mc6, eps = 0.65, minPts = 20)
mc6df = data.frame(mc6, cluster = as.factor(res$cluster))
table(mc6df$cluster)

#plot
ggplot(mc6df, aes(x=CD4,    y=CD8,  col=cluster))+geom_density2d()
ggplot(mc6df, aes(x=CD3all, y=CD20, col=cluster))+geom_density2d()
```

Vary *eps* so that there are 4 clusters, where at least 2 of them have more than 100 points
```{r}
mc6 = Biobase::exprs(fcsBT)[, c(15, 16, 19, 33, 25, 40)]
res = dbscan::dbscan(mc6, eps = 0.95, minPts = 20)
mc6df = data.frame(mc6, cluster = as.factor(res$cluster))
table(mc6df$cluster)

#plot
ggplot(mc6df, aes(x=CD4,    y=CD8,  col=cluster))+geom_density2d()
ggplot(mc6df, aes(x=CD3all, y=CD20, col=cluster))+geom_density2d()
```

Increase dimension to 7

```{r}
mc7 = Biobase::exprs(fcsBT)[, c(11, 15, 16, 19, 25, 33, 40)]
res = dbscan::dbscan(mc7, eps = 0.95, minPts = 20)
mc7df = data.frame(mc7, cluster = as.factor(res$cluster))
table(mc7df$cluster)

#plot
ggplot(mc7df, aes(x=CD4,    y=CD8,  col=cluster))+geom_density2d()
ggplot(mc7df, aes(x=CD3all, y=CD20, col=cluster))+geom_density2d()
```

How does dbscan work?
- Cluster points in dense regions according to the density-connected-ness criterion
- Looks for high density points in a neighbourhood
- Must statisfy a number of properties to be considered a cluster

## Hierarchical Clustering

**Question 5.8**

```{r}
load("/MBIO7160/ModernStatsforModernBiol/RData/Morder.RData")
Morder

#Eucliden Distance
pheatmap(Morder, cluster_rows = TRUE,
  cluster_cols = TRUE, clustering_distance_rows = "euclidean",
  clustering_distance_cols = "euclidean", color = inferno(20))

#Manhattan Distance
pheatmap(Morder, cluster_rows = TRUE,
  cluster_cols = TRUE, clustering_distance_rows = "manhattan",
  clustering_distance_cols = "manhattan", color = inferno(20))

```
**Question 5.9**

The Euclidean distance has approx two main clusters of approx equal size that are subdivided into multiple smaller clusters whereas the Manhattan is initally subdivided into one smaller cluster and one larger cluster (of which are subdivided into smaller clusters).

**Question 5.10**
There are at least 9 different ways to order the tip labels.

## Validating and Choosing the # of Clusters

WSS
- Within-group sum of squares

BSS
- Between groups sum of squares

When attempting to decide how many clusters are appropriate for the data, it can be useful to look at cases where the answer is actually known.

Simulate data from four groups...

```{r}
library("dplyr")
simdat = lapply(c(0, 8), function(mx) {
  lapply(c(0,8), function(my) {
    tibble(x = rnorm(100, mean = mx, sd = 2),
           y = rnorm(100, mean = my, sd = 2),
           class = paste(mx, my, sep = ":"))
   }) %>% bind_rows
}) %>% bind_rows
simdat
simdatxy = simdat[, c("x", "y")]
```
Plot the data and observe the four clusters
```{r}
ggplot(simdat, aes(x = x, y = y, col = class)) + geom_point() +
  coord_fixed()
```
Compute the within-groups sum of squares for the clusters obtained from the k-means method

```{r}
wss = tibble(k = 1:8, value = NA_real_)
wss$value[1] = sum(scale(simdatxy, scale = FALSE)^2)
for (i in 2:nrow(wss)) {
  km  = kmeans(simdatxy, centers = wss$k[i])
  wss$value[i] = sum(km$withinss)
}
wss
ggplot(wss, aes(x = k, y = value)) + geom_col()
```



The K in K-Means denotes the number of clusters. This algorithm is bound to converge to a solution after some iterations. It has 4 basic steps:

1. Initialize Cluster Centroids (Choose those 3 books to start with)

2. Assign datapoints to Clusters (Place remaining the books one by one)

3. Update Cluster centroids (Start over with 3 different books)

4. Repeat step 2–3 until the stopping condition is met.


**Question 5.12**
Run the code above and compare.  Why are the values different?

(a) The values differ slightly between runs...
Talk about bookstore example to explain k means...

Really good resource explaining k-means
https://towardsdatascience.com/k-means-clustering-from-a-to-z-f6242a314e9a

(b) Compute the WSS value for uniform data.

```{r}
simdat2 = lapply(c(0, 8), function(mx) {
  lapply(c(0,8), function(my) {
    tibble(x = runif(100, mean = mx, sd = 2),
           y = runif(100, mean = mx, sd = 2),
           class = paste(mx, my, sep = ":"))
   }) %>% bind_rows
}) %>% bind_rows
simdat2
simdatxy = simdat2[, c("x", "y")]


#plot the data
wss = tibble(k = 1:8, value = NA_real_)
wss$value[1] = sum(scale(simdatxy, scale = FALSE)^2)
for (i in 2:nrow(wss)) {
  km  = kmeans(simdatxy, centers = wss$k[i])
  wss$value[i] = sum(km$withinss)
}
wss
ggplot(wss, aes(x = k, y = value)) + geom_col()
```

k=4 still appears to be the best choice for the number of clusters for the same range and dimensions.

##Calinski-Harabasz

```{r}
library("fpc")
library("cluster")
CH = tibble(
  k = 2:8,
  value = sapply(k, function(i) {
    p = pam(simdatxy, i)
    calinhara(simdatxy, p$cluster)
  })
)
ggplot(CH, aes(x = k, y = value)) + geom_line() + geom_point() +
  ylab("CH index")
```
k=4 still appears to be the best cluster choice.


##Gap Statistic

Compute the log of WSS for a range of values of k (# of clusters) and compare it to that obtained on reference data of similar dimensions with various possible "non-clustered distributions.

**Question 5.14**
Make a function that plots the gap statistic and show the output for simdat

```{r}
library("cluster")
library("ggplot2")
pamfun = function(x, k)
  list(cluster = pam(x, k, cluster.only = TRUE))

gss = clusGap(simdatxy, FUN = pamfun, K.max = 8, B = 50,
              verbose = FALSE)
plot_gap = function(x) {
  gstab = data.frame(x$Tab, k = seq_len(nrow(x$Tab)))
  ggplot(gstab, aes(k, gap)) + geom_line() +
    geom_errorbar(aes(ymax = gap + SE.sim,
                      ymin = gap - SE.sim), width=0.1) +
    geom_point(size = 3, col=  "red")
}
plot_gap(gss)
```

Example with real data

Load the data...
```{r}
library("Hiiragi2013")
data("x")
```
Choose the 50 most variable genes...

```{r}
selFeats = order(rowVars(Biobase::exprs(x)), decreasing = TRUE)[1:50]
embmat = t(Biobase::exprs(x)[selFeats, ])
embgap = clusGap(embmat, FUN = pamfun, K.max = 24, verbose = FALSE)
k1 = maxSE(embgap$Tab[, "gap"], embgap$Tab[, "SE.sim"])
k2 = maxSE(embgap$Tab[, "gap"], embgap$Tab[, "SE.sim"],
           method = "Tibs2001SEmax")
c(k1, k2)
```
Above code chunk reports a cluster size of *k* = 9 (i.e., gap is not larger than the first local maximum minus a standard error) and *k* = 7 (the smallest k)

View manual page for clusGap

```{r}
?clusGap
```

Plot the gap statistic

```{r}
plot(embgap, main = "")
cl = pamfun(embmat, k = k1)$cluster
table(pData(x)[names(cl), "sampleGroup"], cl)
```

**Question 5.15**
How do the results change if you use all the features in x?

```{r}
selFeats2 = Biobase::exprs(x)
selFeats2
embmat2 = Biobase::exprs(x)
embgap2 = clusGap(embmat2, FUN = pamfun, K.max = 24, verbose = FALSE)
k3 = maxSE(embgap2$Tab[, "gap"], embgap2$Tab[, "SE.sim"])
k4 = maxSE(embgap2$Tab[, "gap"], embgap2$Tab[, "SE.sim"],
           method = "Tibs2001SEmax")
c(k3, k4)


#Plot
plot(embgap2, main = "")
cl = pamfun(embmat2, k = k3)$cluster
table(pData(x)[names(cl), "sampleGroup"], cl)
```
...Takes forever to run...



##Cluster Validation using the Bootstrap

- Check for cluster stability over time

Use the Hiiragi2013 data to test the following hypothesis:

The inner cell mass (ICM) of the mouse blastocyst on embryonic day 3.5 (E3.5) falls into two clusters corresponding to pluripotent epiblast (EPI) vs. primitive endoderm (PE) and embryonic day 3.25 (E3.25) does not demonstrate this.

```{r}
clusterResampling = function(x, ngenes = 50, k = 2, B = 250,
                             prob = 0.67) {
  mat = Biobase::exprs(x)
  ce = cl_ensemble(list = lapply(seq_len(B), function(b) {
    selSamps = sample(ncol(mat), size = round(prob * ncol(mat)),
                      replace = FALSE)
    submat = mat[, selSamps, drop = FALSE]
    sel = order(rowVars(submat), decreasing = TRUE)[seq_len(ngenes)]
    submat = submat[sel,, drop = FALSE]
    pamres = pam(t(submat), k = k)
    pred = cl_predict(pamres, t(mat[sel, ]), "memberships")
    as.cl_partition(pred)
  }))
  cons = cl_consensus(ce)
  ag = sapply(ce, cl_agreement, y = cons)
  list(agreements = ag, consensus = cons)
}
```
...good explaination of the code chunk above is on pg. 128

Calculate the Euclidean dissimilarity

```{r}
iswt = (x$genotype == "WT")
cr1 = clusterResampling(x[, x$Embryonic.day == "E3.25" & iswt])
cr2 = clusterResampling(x[, x$Embryonic.day == "E3.5"  & iswt])
```
Create a tibble

```{r}
ag1 = tibble(agreements = cr1$agreements, day = "E3.25")
ag2 = tibble(agreements = cr2$agreements, day = "E3.5")

#Beeswarm
ggplot(bind_rows(ag1, ag2), aes(x = day, y = agreements)) +
  geom_boxplot() +
  ggbeeswarm::geom_beeswarm(cex = 1.5, col = "#0000ff40")
```
Based on the plot created above, 1 indicates perfect agreement and lower values indicate less agreement.

Plot the Membership probabilities of the consensus clustering

```{r}
#Facet
mem1 = tibble(y = sort(cl_membership(cr1$consensus)[, 1]),
              x = seq(along = y), day = "E3.25")
mem2 = tibble(y = sort(cl_membership(cr2$consensus)[, 1]),
              x = seq(along = y), day = "E3.5")
ggplot(bind_rows(mem1, mem2), aes(x = x, y = y, col = day)) +
  geom_point() + facet_grid(~ day, scales = "free_x")
```

For E3.25, the probabilities are diffuse indicating that the individual clusterings have low agreement.

The distribution is bimodal for E3.5 with only one ambiguous sample.

##Clustering as a Means for Denoising

Example of noisy observations with different baselines frequencies

seq1 = 10^3
seq2 = 10^5

```{r}
seq1 = rmvnorm(n = 1e3, mu = -c(1, 1), sigma = 0.5 * diag(c(1, 1)))
seq2 = rmvnorm(n = 1e5, mu =  c(1, 1), sigma = 0.5 * diag(c(1, 1)))
twogr = data.frame(
  rbind(seq1, seq2),
  seq = factor(c(rep(1, nrow(seq1)),
                 rep(2, nrow(seq2))))
)
colnames(twogr)[1:2] = c("x", "y")
ggplot(twogr, aes(x = x, y = y, colour = seq,fill = seq)) +
  geom_hex(alpha = 0.5, bins = 50) + coord_fixed()
```

**Question 5.16**
Cluster the two groups according to distance from the group center

```{r}

```

Task (page130)

```{r}
#Simulate n=2000 binary variables of length 200 and a sequencing errors = 0.001

n    = 2000
len  = 200
perr = 0.001
seqs = matrix(runif(n * len) >= perr, nrow = n, ncol = len)

seqs

#Compute pairwise distances

dists = as.matrix(dist(seqs, method = "manhattan"))

#Maximum distance within a set of reads

dfseqs = tibble(
  k = 10 ^ seq(log10(2), log10(n), length.out = 20),
  diameter = vapply(k, function(i) {
    s = sample(n, i)
    max(dists[s, s])
    }, numeric(1)))
ggplot(dfseqs, aes(x = k, y = diameter)) + geom_point()+geom_smooth()
```

De-noising 16S rRNA

**Question 5.17**
What is the distribution for two sequences of length 200 with technlogical sequencing errors occuring at 0.0005.

Use Monte Carlo Simulation...

```{r}
simseq10K = replicate(1e5, sum(rpois(200, 0.0005)))
mean(simseq10K)
vcd::distplot(simseq10K, "poisson")
```

## Inferring Sequence Variants

DADA
- Divisive Amplicon Denoising Algorithm
  - Distinguishes sequencing errors from real biological variation
  - Model parameters are estimated from the data using an EM-type approach
  
```{r}
derepFs = readRDS(file="/MBIO7160/ModernStatsforModernBiol/RData/derepFs.rds")
derepRs = readRDS(file="/MBIO7160/ModernStatsforModernBiol/RData/derepRs.rds")

ddF = dada(derepFs, err = NULL, selfConsist = TRUE)
ddR = dada(derepRs, err = NULL, selfConsist = TRUE)
```

Verify the error transition rates have been well estimated by plotting the frequencies of each type of nucleotide transition as a function of quality.

```{r}

#Plot errors

plotErrors(ddF)
plotErrors(ddR)

```
  
Re-run algorithm on data to find sequence variants.

```{r}
dadaFs = dada(derepFs, err=ddF[[1]]$err_out, pool = TRUE)
dadaRs = dada(derepRs, err=ddR[[1]]$err_out, pool = TRUE)
```

Merge the inferred forward and reverse sequence.

```{r}
mergers = mergePairs(dadaFs, derepFs, dadaRs, derepRs)
```

Generate a contingency table of counts of ASVs.

```{r}
seqtab.all = makeSequenceTable(mergers[!grepl("Mock",names(mergers))])
```

**Question 5.18**

Explore the components of dadaRs and mergers

```{r}
length(dadaRs) # 20 elements in total showing the denoising results

names(dadaRs)

length(mergers) # 20 elements
names(mergers)
```

Chimera...
- Chimeras are artifact sequences formed by two or more biological sequences incorrectly joined together.

Remove chimerias

```{r}
seqtab = removeBimeraDenovo(seqtab.all)
```

**Question 5.19**
(a) Why are chimera easy to remove?

Produce spurious OTUs (Operational Taxonomic Units)

(b) What proportion of reads were chimeric in seqtab.all data?

```{r}
Before = length(seqtab.all)

After = length(seqtab)

Total = (Before - After) / (Before) 
Total

```
2812 sequences were removed = ~35%

(c) What proportion of unique sequence variants are chimeric?

```{r}
UniqB = sum(unique(seqtab.all))
UniqB

UniqA = sum(unique(seqtab))
UniqA

TotalUni = (UniqB - UniqA) / (UniqB)
TotalUni

```

7% of unique sequence variants were chimeric.

**Exercises**

**5.1**

(a) Compute the silhouette index for simdat

The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate.

```{r}
# k = 4, Avg Silhouette = 0.52

pam4 = pam(simdatxy, 4)
sil4 = silhouette(pam4, 4)
plot(sil4, col=c("red","green","blue","purple"), main="Silhouette")
```
(b) Change the number of clusters *k*

```{r}
# k = 5, Avg Silhouette = 0.46

pam5 = pam(simdatxy, 5)
sil5 = silhouette(pam5, 5)
plot(sil5, col=c("red","green","blue","purple"), main="Silhouette")

# k = 6 , Avg Silhouette = 0.41

pam6 = pam(simdatxy, 6)
sil6 = silhouette(pam6, 6)
plot(sil6, col=c("red","green","blue","purple"), main="Silhouette")

# k = 2, Avg Silhouette = 0.33

pam2 = pam(simdatxy, 2)
sil2 = silhouette(pam2, 2)
plot(sil2, col=c("red","green","blue","purple"), main="Silhouette")

# k = 11, Avg Silhouette = 0.43

pam11 = pam(simdatxy, 11)
sil11 = silhouette(pam11, 11)
plot(sil11, col=c("red","green","blue","purple"), main="Silhouette")
```

Based on the results above, it appears *k* = 4 gives the best results.

(c) Repeat for groups with a uniform distribution over a range of values.

```{r}
simdat2 = lapply(c(0, 8), function(mx) {
  lapply(c(0,8), function(my) {
    tibble(x = runif(100, min = 1, max = 100),
           y = runif(100, min = 1, max = 100),
           class = paste(mx, my, sep = ":"))
   }) %>% bind_rows
}) %>% bind_rows
simdat2
simdatxy2 = simdat2[, c("x", "y")]

# k = 125

pam2 = pam(simdatxy2, 60)
sil2 = silhouette(pam2, 60)
plot(sil2, col=c("red","green","blue","purple"), main="Silhouette")

```
Hard to get Avg Sil width close to 1...

**Exercise 5.2**

(a) Make a character representation of the distance between the 20 locations in the dune data from the vegan package using the function *symnum*

- The R function symnum can be used to easily highlight the highly correlated variables. It replaces correlation coefficients by symbols according to the value.



```{r}
library(vegan)
data("dune")

dune

corMat = cor(dune)
corMat

#symnum = Symbolic Number Coding
?symnum
symnum(corMat, abbr.colnames = FALSE)
```
(b) Make a heatmap of these distances

```{r}
#heatmap
pheatmap(corMat, cluster_rows = FALSE, cluster_cols = FALSE, color = inferno(20))
```

**5.3**

Need to look at what Spiral Graphs are used for....

(a) Plot 
```{r}

library("kernlab")
data("spirals")
clusts = kmeans(spirals,2)$cluster
plot(spirals, col = c("blue", "red")[clusts])
```

(b) Repeat using dbscan

```{r}
data("spirals", package = "kernlab")
res.dbscan = dbscan::dbscan(spirals, eps = 0.16, minPts = 3)
plot(spirals,col=c("blue","red","forestgreen")[res.dbscan$cluster])
```

(c) Repeat with different parameters

```{r}
res.dbscan = dbscan::dbscan(spirals, eps = 0.10, minPts = 6)
plot(spirals,col=c("blue","red","forestgreen")[res.dbscan$cluster])
```
The number of groups does not seem to be very robust...  

**5.4**

https://www.huffpost.com/entry/post_b_817254

What are some reasons for the clustering and high incidence rates on the West and East Coasts and around Chicago?

Underreporting?  Geographical distribution of susceptible populations?

**5.5**

```{r}
base_dir = "/MBIO7160/ModernStatsforModernBiol/RData/"
miseq_path = file.path(base_dir, "MiSeq_SOP")
filt_path = file.path(miseq_path, "filtered")
fnFs = sort(list.files(miseq_path, pattern="_R1_001.fastq"))
fnRs = sort(list.files(miseq_path, pattern="_R2_001.fastq"))
sampleNames = sapply(strsplit(fnFs, "_"), `[`, 1)
if (!file_test("-d", filt_path)) dir.create(filt_path)
filtFs = file.path(filt_path, paste0(sampleNames, "_F_filt.fastq.gz"))
filtRs = file.path(filt_path, paste0(sampleNames, "_R_filt.fastq.gz"))
fnFs = file.path(miseq_path, fnFs)
fnRs = file.path(miseq_path, fnRs)

print(length(fnFs))
```

Sequence Quality Plots

```{r}
plotQualityProfile(fnFs[1:2]) + ggtitle("Forward")
plotQualityProfile(fnRs[1:2]) + ggtitle("Reverse")
```
The lines show positional summary stats; green is the mean, organge is the median and the dashed organge lines are the 25th and 75th percentile.

**Exercise 5.6**
Generate similar plots for four randomly selected sets of forward and reverse reads.

```{r}
plotQualityProfile(fnFs[7:8]) + ggtitle("Forward")
plotQualityProfile(fnRs[7:8]) + ggtitle("Reverse")

plotQualityProfile(fnFs[9:10]) + ggtitle("Forward")
plotQualityProfile(fnRs[9:10]) + ggtitle("Reverse")

plotQualityProfile(fnFs[11:12]) + ggtitle("Forward")
plotQualityProfile(fnRs[11:12]) + ggtitle("Reverse")
```
Reverse reads tend to be of lower quality.

**5.7**


```{r}

filtF8 <- tempfile(fileext=".fastq.gz")
filtR8 <- tempfile(fileext=".fastq.gz")

filterAndTrim(fwd=fnFs[8], filt=filtF8, rev=fnRs[8], filt.rev=filtR8,
                  trimLeft=10, truncLen=c(240, 160), 
                  maxN=0, maxEE=2)

#Plot Filtered Data

plotQualityProfile(filtF8) + ggtitle("Forward")
plotQualityProfile(filtR8) + ggtitle("Reverse")

#Compare to unfiltered data
plotQualityProfile(fnFs[8]) + ggtitle("Forward")
plotQualityProfile(fnRs[8]) + ggtitle("Reverse")

```
The filterAndTrim(...) function filters the forward and reverse reads jointly, outputting only those pairs of reads that both pass the filter. 

In this function call we did four things: We removed the first trimLeft=10 nucleotides of each read. We truncated the forward and reverse reads at truncLen=c(240, 160) nucleotides respectively. 

We filtered out all reads with more than maxN=0 ambiguous nucleotides. And we filtered out all reads with more than two expected errors (maxEE=2). 